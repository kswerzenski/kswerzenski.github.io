{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "129c588a",
   "metadata": {},
   "source": [
    "Kristen Swerzenski\n",
    "\n",
    "DSC 670 Advanced Uses of Generative AI\n",
    "\n",
    "9 February 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a62929",
   "metadata": {},
   "source": [
    "## Project Milestone 3: Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbfd6e2",
   "metadata": {},
   "source": [
    "The time has come to fine-tune our first model! In the last milestone, I tested a large language model's ability to to serve as a virtual focus group for aspiring screenwriters to gain real-time feedback on their stories, characters, and overall narrative structure with only prompt direction and no fine-tuning of the model. While the model could generate some useful feedback, it often very general and lacked depth and variety. Many responses were repetitive across different prompts, failing to capture the nuances of individual scenes. It was alos observed that the models struggled to interpret the format of a script and could not always decipher dialogue, scene direction, and other elements of the scripts. To improve the quality and specificity of the feedback, we will now employ fine-tuning to further specialize the model's ability to perform this task. By training the model on structured examples of high-quality script analysis, I aim to create a tool that provides more insightful, engaging, and constructive feedback tailored to each unique script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8da18e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70bf6016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading environment variables\n",
    "load_dotenv(dotenv_path=r'C:\\Users\\krist\\Documents\\Data Science MS\\DSC670\\Week 7\\OPENAI_API_KEY.env')\n",
    "\n",
    "# Retrieving the API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "018459fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking response from API\n",
    "if api_key is None:\n",
    "    raise Exception(\"Missing API key.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a61d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the client\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7682f6",
   "metadata": {},
   "source": [
    "### Preparing a data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d87ce5",
   "metadata": {},
   "source": [
    "The first thing that we need to do in order to fine tune the model is locate a data set. However, the data set has to be in a specific format for fine tuning, and despite a lot of searching there are no data sets that I can find that have a structure of input/output pairs that would match what I want the fine-tuned model to achieve (the input would be a script, and the output would be constructive criticism on that script). I was, however, able to find a dataset on Kaggle that contains 33 annotated screenplays for the model to use to better understand the structure of scripts (https://www.kaggle.com/datasets/gufukuro/movie-scripts-corpus). From here, I also need some solid feedback for these scripts that the model can use to understand the type and content of feedback that I would be looking for.\n",
    "\n",
    "To do this, I am first going to use generative AI to take each of the 34 script snippets and provide feedback on the clarity, pacing, and dialogue of each one. While I know that using generative AI to create data for it to essentially train itself on could lead to a self-destructive loop, I'm hoping that it is a solid enough of a foundation for the fine-tuned model to build off of. Depending on time, I might also enter some manual feedback into the training data for certain examples or modify what the LLM produces to introduce some variety into the training set and give that human touch. While it is by no means a perfect means of curating a dataset for fine-tuning, it is the best that can be done with the time and resources available. With unlimited time, I would ideally love to take either take critical reviews of specific parts of films and pair them with their appropraite script or solicit real focus group feedback on different scripts to use for training. For now though, we will let the generative AI generate some feedback for these scripts to construct our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f3902c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scene_heading: INT.  CONCOURSE/AIRPORT TERMINAL - BAY\n",
      "\n",
      "text: CLOSE ON A FACE.  A nine year old boy, YOUNG COLE, his eyes wide\n",
      "\twith wonder. watching something intently.  We HEAR the sounds of\n",
      "\tthe P.A. SYSTEM droning Flight Information mingled with the\n",
      "\tsounds of urgent SHOUTS, running FEET, EXCLAMATIONS.\n",
      "\tYOUNG COLE'S POV:  twenty yards away, a BLONDE MAN is sprawled on\n",
      "\tthe floor, blood oozing from his gaudy Hawaiian shirt.\n",
      "\tA BRUNETTE in a tight dress, her face obscured from YOUNG COLE'S\n",
      "\tvie\n"
     ]
    }
   ],
   "source": [
    "# Loading in all of the annotated scripts\n",
    "scripts_dir = 'manual_annotations'\n",
    "\n",
    "# Reading all text files in the directory\n",
    "script_files = [f for f in os.listdir(scripts_dir) if f.endswith('.txt')]\n",
    "\n",
    "# Creating a dictionary to store the scene data\n",
    "scenes = {}\n",
    "\n",
    "# Looping over all script files and read their content\n",
    "for script_file in script_files:\n",
    "    with open(os.path.join(scripts_dir, script_file), 'r', encoding='utf-8') as f:\n",
    "        scenes[script_file] = f.read()\n",
    "\n",
    "# Checking the first few lines of one scene to verify\n",
    "print(scenes[script_files[0]][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2581c4",
   "metadata": {},
   "source": [
    "Now that the scripts are loaded into a dictionary, I am going to create a function to make calls to the API to retrieve feedback in the format I am looking for for wach script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bc5097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to generate feedback for each script\n",
    "def generate_feedback(scene_text):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": \"You are a professional screenplay analyst. Given the following scene, provide feedback on its clarity, pacing, and dialogue.\"},\n",
    "            {\"role\": \"user\", \"content\": scene_text},\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    feedback = completion.choices[0].message.content\n",
    "    return feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6ff0acc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Feedback on Scene:**\n",
      "\n",
      "**Clarity:**\n",
      "The scene effectively establishes a vivid contrast between two distinct settings: the airport terminal and the dystopian future world. The transition from Young Cole's perspective to the adult Cole's environment is clear, and the use of visual and auditory cues helps the audience follow the narrative shift. However, the scene could benefit from more explicit connections between the two timelines to enhance the audience's understanding of their relationship. The introduction of the scientists and their roles could be clarified further to avoid confusion about their significance and purpose.\n",
      "\n",
      "**Pacing:**\n",
      "The pacing of the scene is generally well-managed, with a gradual build-up from the initial airport incident to the more intense and mysterious future setting. The transition from the dream-like memory to the harsh reality of the future is smooth, maintaining the audience's engagement. However, the sequence in the future world, particularly the decontamination and specimen collection scenes, could be tightened to maintain momentum. Some of the detailed descriptions, while atmospheric, may slow down the narrative progression.\n",
      "\n",
      "**Dialogue:**\n",
      "The dialogue is functional and serves to reveal character and plot, but it could be more dynamic to enhance character development and engagement. The exchanges between Cole and the guards, as well as the scientists, are straightforward but lack emotional depth or tension that could add layers to Cole's character. The scientists' dialogue, while informative, could be more varied to distinguish their individual personalities and expertise. Additionally, the use of overlapping voices at the end is an interesting technique but might be overwhelming if not balanced with clearer individual lines to maintain coherence.\n",
      "\n",
      "Overall, the scene effectively sets up a mysterious and intriguing narrative, but could benefit from tighter pacing and more nuanced dialogue to fully engage the audience and clarify the connections between the past and future timelines.\n"
     ]
    }
   ],
   "source": [
    "# Applying the function to every script in the dictionary and printing the first one\n",
    "feedback_data = {}\n",
    "for script_file, scene_text in scenes.items():\n",
    "    feedback = generate_feedback(scene_text)\n",
    "    feedback_data[script_file] = {\n",
    "        \"scene_text\": scene_text,\n",
    "        \"feedback\": feedback\n",
    "    }\n",
    "    \n",
    "    \n",
    "print(feedback_data[script_files[0]]['feedback'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858567e7",
   "metadata": {},
   "source": [
    "The feedback that theh LLM returned is overall pretty solid. It is definitely more detailed than some of the prompts I tested previously thanks to the defined structure of responses I asked for. Now I am going to take all of these generated feedback responses and format them first into a JSON, then formal the JSON into a JSONL for training purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5de3af2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback data saved as 'feedback_data.json'\n"
     ]
    }
   ],
   "source": [
    "# Saving input/response pairs as a JSON\n",
    "with open('feedback_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(feedback_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Feedback data saved as 'feedback_data.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d37938ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning dataset saved as 'fine_tuning_data.jsonl'\n"
     ]
    }
   ],
   "source": [
    "# Converting JSON into JSONL\n",
    "with open('feedback_data.json', 'r', encoding='utf-8') as f:\n",
    "    feedback_data = json.load(f)\n",
    "\n",
    "# Defining the system instruction\n",
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a focus group reviewing a scene script for an unproduced film or television show. Provide constructive feedback on clarity, pacing, and dialogue. Consider engagement, emotional impact, and storytelling effectiveness in your analysis.\"\n",
    "}\n",
    "# Converting and save to JSONL format\n",
    "with open('fine_tuning_data.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for scene_file, details in feedback_data.items():\n",
    "        json.dump({\n",
    "            \"messages\": [\n",
    "                system_message,\n",
    "                {\"role\": \"user\", \"content\": details[\"scene_text\"]},\n",
    "                {\"role\": \"assistant\", \"content\": details[\"feedback\"]}\n",
    "            ]\n",
    "        }, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"Fine-tuning dataset saved as 'fine_tuning_data.jsonl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b618baae",
   "metadata": {},
   "source": [
    "I took a quick look at the JSON an JSONL output and everything seems to have been converted correctly. I also read through some of the generated responses and made a few minor tweaks. I didn't perform any great overhauls on any of the responses due to time constraints, so I think I am going to see how well this mostly-generated trianing data performs when used to tun the model before spending time to manually edit and build out the training data any more. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b3aaaf",
   "metadata": {},
   "source": [
    "### Dataset Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a822f6b2",
   "metadata": {},
   "source": [
    "Now that I have a JSONL training data set in what should be the correct format, I want to make sure that it is in the correct format for fine-tuning. In this section I will be performing a number of checks on the data to ensure it loads correctly, it is formatted correctly, and to get some estimates of the computational resources that will be required for the training job. This will include basic checks looking for any glaring overall issues with the data set, formattting checks which will ensure that all of the examples in the dataset are appropriately formatted for training, and some cost estimations to better understand the structure of the data in terms of tokens, costs, and distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a314660",
   "metadata": {},
   "source": [
    "#### Basic Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcd06709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to open the training data and perform basic checks\n",
    "def basic_checks(data_file):\n",
    "    try:\n",
    "        with open(data_file, 'r', encoding='utf-8') as f:\n",
    "            dataset = [json.loads(line) for line in f]\n",
    "            \n",
    "        print(f\"Basic checks for file {data_file}:\")\n",
    "        print(\"Count of examples in training dataset:\", len(dataset))\n",
    "        print(\"First example:\")\n",
    "        for message in dataset[0]['messages']:\n",
    "            print(message)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"An error has occurred in file {data_file}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c15e2f",
   "metadata": {},
   "source": [
    "#### Formatting Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4bd687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the data for consistent formatting\n",
    "def format_checks(dataset, filename):\n",
    "    format_errors = defaultdict(int)\n",
    "    \n",
    "    for ex in dataset:\n",
    "        # Checking to ensure each example is a dictionary\n",
    "        if not isinstance(ex, dict):\n",
    "            format_errors[\"data_type\"] += 1\n",
    "            continue\n",
    "        \n",
    "        # Ensuring each example has a 'messages' key\n",
    "        messages = ex.get(\"messages\", None)\n",
    "        if not messages:\n",
    "            format_errors[\"missing_messages_list\"] += 1\n",
    "            continue\n",
    "        \n",
    "        for message in messages:\n",
    "            # Ensuring each message has a role and content key\n",
    "            if \"role\" not in message or \"content\" not in message:\n",
    "                format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        # Checking for any inrecognized keys\n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        # Ensuring the role of message is one of the recognized roles\n",
    "        if message.get(\"role\", None) not in (\n",
    "            \"system\",\n",
    "            \"user\",\n",
    "            \"assistant\",\n",
    "            \"function\"\n",
    "        ):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        # Checking for either a content or function call in each message, and that they're formatted appropriately    \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "              \n",
    "        # Ensuring at least one message has the tole of assistant        \n",
    "        if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "            format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "    # If format errors are found, print them and return False\n",
    "    if format_errors:\n",
    "        print(f\"Formatting errors found in file {filename}:\")\n",
    "        for k, v in format_errors.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "        return False\n",
    "    print(f\"No formatting errors found in file {filename}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5627f09f",
   "metadata": {},
   "source": [
    "#### Cost Estimation and Token Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d7a70e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token and epoch estimates\n",
    "MAX_TOKENS = 4096\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "# Estimating how many tokens will be used for training\n",
    "def estimate_tokens(dataset, assistant_tokens):\n",
    "    n_epochs = TARGET_EPOCHS\n",
    "\n",
    "    # Retrieving number of examples in the dataset\n",
    "    n_train_examples = len(dataset)\n",
    "\n",
    "    # Adjusting epochs if number of examples is less than the target\n",
    "    if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "        n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "    \n",
    "    # Adjusting the epochs if the number of examples is more than the target\n",
    "    elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "        n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "    # Calculating the total number of tokens in the dataset\n",
    "    n_billing_tokens_in_dataset = sum(min(MAX_TOKENS, length) for length in assistant_tokens)\n",
    "\n",
    "    # Printing the total token count that will be charged during training\n",
    "    print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "\n",
    "    # Print default number of epochs for training\n",
    "    print(f\"You will train for {n_epochs} epochs on this dataset\")\n",
    "\n",
    "    # Printing total number of tokens that will be charged during training\n",
    "    print(f\"You will be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "\n",
    "    # If the total token count exceeds the maximum tokens, print a warning \n",
    "    if n_billing_tokens_in_dataset > MAX_TOKENS:\n",
    "        print(\"WARNING: Your dataset contains examples longer than 4K tokens by {n_billing_tokens_in_dataset - MAX_TOKENS} tokens.\")\n",
    "        print(\"You will be charged for the full length of these examples during training, but only the first 4K tokens will be used for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4534c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the number of tokens in the messages\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b886d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the number of tokens in the assistant messages\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09a376a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the distribution of values\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03305721",
   "metadata": {},
   "source": [
    "#### Performing the Data Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13edbc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic checks for file fine_tuning_data.jsonl:\n",
      "Count of examples in training dataset: 33\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a focus group reviewing a scene script for an unproduced film or television show. Provide constructive feedback on clarity, pacing, and dialogue. Consider engagement, emotional impact, and storytelling effectiveness in your analysis.'}\n",
      "{'role': 'user', 'content': 'scene_heading: INT.  CONCOURSE/AIRPORT TERMINAL - BAY\\n\\ntext: CLOSE ON A FACE.  A nine year old boy, YOUNG COLE, his eyes wide\\n\\twith wonder. watching something intently.  We HEAR the sounds of\\n\\tthe P.A. SYSTEM droning Flight Information mingled with the\\n\\tsounds of urgent SHOUTS, running FEET, EXCLAMATIONS.\\n\\tYOUNG COLE\\'S POV:  twenty yards away, a BLONDE MAN is sprawled on\\n\\tthe floor, blood oozing from his gaudy Hawaiian shirt.\\n\\tA BRUNETTE in a tight dress, her face obscured from YOUNG COLE\\'S\\n\\tview, rushes to the injured man, kneels beside him, ministering\\n\\tto his wound.\\n\\tANGLE ON YOUNG COLE, flanked by his PARENTS, their faces out of\\n\\tview, as they steer him away.\\n\\nspeaker_heading: FATHER\\'S VOICE (o.s.)\\n\\ndialog: Come on, Son --this is no place for us.\\n\\ntext: YOUNG COLE resists momentarily, mesmerized by the drama.\\n\\tYOUNG COLE\\'S POV:  intermittently visible through a confusion of\\n\\tFIGURES rushing through the foreground, the BLONDE MAN reaching\\n\\tup and touching the cheek of the kneeling BRUNETTE in a gesture\\n\\tof enormous tenderness, a gesture of farewell, while the P.A.\\n\\tSYSTEM continues its monotonous monotone...\\n\\nspeaker_heading: P.A. SYSTEM\\n\\ndialog: Flight 784 for San Francisco is now\\n\\t\\tready for boarding at inmate number\\n\\t\\t66578, Greely.\\n\\nscene_heading: INT.  PRISON DORMITORY/FUTURE - ETERNAL NIGHT\\n\\nspeaker_heading: PRISON P.A. SYSTEM\\n\\ndialog: --number 5429, Garcia -- number 87645, Cole...\\n\\ntext: COLE, late thirties, dark hair, comes awake in a bunk cage, one\\n\\tof many stacked four high along both sides of a long dim\\n\\tcorridor.  He blinks in the near dark, shaken, disoriented.\\n\\tThen, as he \"recovers\" from his very vivid dream, WE GET OUR\\n\\tFIRST LOOK AT HIS ENVIRONMENT...A WINDOWLESS UNDERGROUND WORLD OF\\n\\tETERNAL NIGHT SOMETIME IN THE FUTURE...AN ALMOST COLORLESS\\n\\t\"REALITY\" OF BLURRED EDGES AND ECHOEY SOUNDS, MUCH MORE\\n\\t\"DREAMLIKE\" THAN HIS DREAM.\\n\\tFlashlights glare. In the half-light, COLE sees spooky figures,\\n\\tGUARDS, moving among the locked bunk/cages.\\n\\tCOLE turns and whispers to the occupant of the next cage, JOSE...\\n\\nspeaker_heading: COLE\\n\\ndialog: Ssssst!  Jose, what\\'s going on?\\n\\ntext: JOSE\\'s face is almost lost in shadow.  What there is of it is\\n\\tyouthful.  He\\'s just a scared Puerto Rican kid!\\n\\nspeaker_heading: JOSE\\n\\ndialog: \"Volunteers\" again.\\n\\ntext: JOSE immediately rolls over and feigns sleep as SCARFACE, a\\n\\tmenacing guard with a jagged scar running down his cheek, looms\\n\\tclose to COLE\\'s cage and unlocks it.\\n\\nspeaker_heading: SCARFACE\\n\\ndialog: \"Volunteer duty\".\\n\\ntext: The PRISONERS in the other cages watch silently with narrowed eyes.\\n\\nspeaker_heading: COLE\\n\\ndialog: I didn\\'t volunteer.\\n\\nspeaker_heading: SCARFACE\\n\\ndialog: You causing trouble again?\\n\\nspeaker_heading: COLE\\n\\ndialog: (controls his temper)\\n\\t\\tNo trouble.\\n\\nscene_heading: INT.  EQUIPMENT ROOM - ETERNAL NIGHT\\n\\ntext: COLE\\'s alone, struggling to get into what looks like a space suit\\n\\tin a room where suits hang like ghosts with blank eyes.\\n\\nscene_heading: TITLES BEGIN SUPERED OVER THE SCENE\\n\\ntext: COLE has the torso of the suit on now and is trying to close it.\\n\\nspeaker_heading: OFFSCREEN VOICE (o.s.)\\n\\ndialog: All openings must be closed.\\n\\ntext: COLE looks for the source of the voice, a tiny grate in the wall.\\n\\nspeaker_heading: OFFSCREEN VOICE (o.s.)\\n\\ndialog: If the integrity of the suit is compromised\\n\\t\\tin any way, if the fabric is torn or a zipper\\n\\t\\tnot closed, readmittance will be denied.\\n\\nscene_heading: INT.  SEALED CHAMBER - MINUTES LATER (ETERNAL NIGHT)\\n\\ntext: COLE, wearing the \"space suit\" and a helmet with a plastic visor,\\n\\tsteps into a tiny chamber, a kind of air lock.  The heavy door\\n\\tclangs shut behind him.  He\\'s alone.  COLE\\'S breath comes quicker\\n\\tnow as he sucks oxygen from the air tanks on his back.\\n\\tOn the opposite wall is another door with a huge wheel lock.\\n\\tCOLE turns the heavy wheel, opens the door, steps through It\\n\\nscene_heading: INT.  ELEVATOR - SECONDS LATER (ETERNAL NIGHT)\\n\\ntext: COLE\\'S in an ascending elevator that groans and creaks.  He looks\\n\\tdown at a crudely drawn map he holds in his gloved hand.\\n\\tThe map shows a series of tunnels and ladders.\\n\\nscene_heading: INT.  SEWER PIPE - MINUTES LATER (NIGHT)\\n\\ntext: COLE pans a flashlight, probing the filthy sewer he\\'s wading through RATS flee the blade of light, scurry across islands of rusting junk.\\n\\tThe flashlight beam settles on a ladder mounted in the wall.\\n\\tReaching the rusted ladder, COLE starts to climb awkwardly.\\n\\nscene_heading: EXT.  CITY STREET/FUTURE - MOMENTS LATER (NIGHT)\\n\\ntext: A SCRAPING NOISE as a heavy man-hole cover is pushed up and moved\\n\\taside.  COLE\\'S helmeted head emerges from below.\\n\\tCOLE\\'S POV THROUGH HIS PLASTIC-VISORED HELMET:  a city in\\n\\tmoonlight!  A surreal image of abandoned buildings.  No people\\n\\tanywhere.  The only sounds are the WIND and COLE\\'S BREATHING.\\n\\nscene_heading: EXT.  ANOTHER CITY STREET - MINUTES LATER (NIGHT)\\n\\ntext: COLE\\'S light reveals abandoned vine-covered automobiles.\\t\\n\\tMoving to the nearest car, COLE searches in the vines for\\n\\tsomething.  Finds it.  An insect.\\n\\tCOLE takes the bug in his gloved hand.  As he clumsily inserts it\\n\\tinto a collection tube, something makes him turn.\\n\\tThere\\'s something across the street in the dark.  Something alive.\\n\\tCOLE points his flashlight and reveals...a BEAR!  Startled by the\\n\\tlight, the animal blinks, then stands on its rear legs and ROARS.\\n\\tANGLE ON COLE, staring wide-eyed.\\n\\tThen, the BEAR sinks down onto all fours and, trying to avoid the\\n\\tflashlight, it pads quickly down the street.\\n\\nscene_heading: INT.  SUBTERRANEAN PARKING GARAGE - NIGHT\\n\\ntext: Using the flashlight to see, COLE reaches down to the cracked\\n\\tfloor and gets another specimen.  DOGSHIT!\\n\\tThe only sound is COLE\\'S labored BREATHING.\\n\\tThen, a different SOUND.  GRRRR!  A dog.  More GRRRRS.  More\\n\\tdogs.  Then, a YIP.  Then, VICIOUS GROWLS.  It\\'s a DOGFIGHT!\\n\\nscene_heading: EXT.  STREET - NIGHT (FIRST LIGHT)\\n\\ntext: A giant OWL, perched on an overhead traffic light, raises its wings\\n\\tand lifts off...rising higher and higher into the brightening sky.\\n\\tBelow, on the street, COLE trudges along, passing deserted\\n\\tbuildings, windows broken, rusted signs dangling.\\n\\nscene_heading: INT.  DEPARTMENT STORE - NIGHT (FIRST LIGHT)\\n\\ntext: COLE\\'S light reveals a spider web just inside the store.  A large\\n\\tSPIDER tries to hide from the light.\\n\\tCOLE reaches carefully into the web and plucks the spider and\\n\\tputs it into one of his specimen tubes.\\n\\tThen, he shines his light all around the once elegant store.  There\\'s\\n\\tnothing but aisle after aisle of moldering consumer goods.\\n\\nscene_heading: EXT.  DEPARTMENT STORE - DAWN\\n\\ntext: As COLE comes out of the store, the first rays of the sun hit the\\n\\tbuilding.  COLE stops, squints into the light through his visor.\\n\\tCOLE\\'S POV:  spray-painted on the wall a long time ago is a stenciled\\n\\tlogo of twelve monkeys holding hands in a circle.  Over it is\\n\\twritten, \"WE DID IT!\"\\n\\tCOLE looks up.\\n\\tCOLE\\'S POV:  high up on a building across the street, a LION\\n\\tpatrols a ledge, pauses, looks out majestically over his world.\\n\\nscene_heading: INT.  FIRST UNDERGROUND DECONTAMINATION CHAMBER - ETERNAL NIGHT\\n\\ntext: ROARING WATER, powerful torrents gushing from nozzles in the\\n\\twall, pummel the still-suited COLE.\\n\\nscene_heading: INT.  SECOND UNDERGROUND DECONTAMINATION CHAMBER - ETERNAL NIGHT\\n\\ntext: Stark naked and shivering, COLE is being scrubbed with brushes on\\n\\tlong poles (like the ones used to wash cars) wielded by two HULKING\\n\\tFIGURES in bulky decontamination suits, their personas lost in their\\n\\twindowed masks.  It\\'s a grim scene in a grim cement room with damp,\\n\\tdripping walls.  From an unseen source comes an AMPLIFIED VOICE,\\n\\nspeaker_heading: AMPLIFIED VOICE (o.s.)\\n\\ndialog: Raise your arms above your head.\\n\\ntext: COLE lifts his arms and the FIGURES start scrubbing his armpits.\\n\\nscene_heading: INT.  TINY CHAMBER - SHORTLY (ETERNAL NIGHT)\\n\\ntext:  Still naked, COLE is seated on a stool while a MASKED TECHNICIAN\\n\\tin a less elaborate, less bulky decontamination outfit draws\\n\\tblood from COLE\\'S arm with an old-fashioned hypodermic needle.\\n\\tCOLE glances toward a single, nearly opaque \"window\" of thick\\n\\tplastic in the rusty iron wall.  VAGUE FIGURES seem to lurk\\n\\tbehind the translucent aperture, studying him.\\n\\tThe TECHNICIAN slips the blood sample through a slot in the wall.\\n\\nscene_heading: INT.  ENGINEERING OFFICE/FUTURE WORLD - ETERNAL NIGHT\\n\\ntext: Ushered in by two guards, TINY and SCARFACE, COLE looks around.\\nCOLE\\'S POV:  wails hidden by old headlines, articles, maps, charts...\\n\\ta blackboard covered with elaborate, sophisticated formulae...surfaces\\n\\theaped with cracked monitors, gerry-rigged computers held together with\\n\\tstring, lasers lost in tangles of cable, ancient tube amplifiers, a\\n\\tdilapidated cardboard reconstruction of a city, stacks of moldering\\n\\tbooks and tattered computer printouts...and, seated at a long conference\\n\\ttable, staring at COLE, six SCIENTISTS:  an ASTROPHYSICIST, ENGINEER,\\n\\tBOTANIST, MICROBIOLOGIST, ZOOLOGIST, and a GEOLOGIST.  They represent\\n\\ta \"modern\" science where brilliant new ideas interface with crude,\\n\\toutdated, patched-together technologies.\\n\\nspeaker_heading: TINY\\n\\ndialog: James Cole.  Cleared from quarantine.\\n\\nspeaker_heading: MICROBIOLOGIST\\n\\ndialog: Thank you.  You two wait outside.\\n\\nspeaker_heading: SCARFACE\\n\\ndialog: He\\'s got a history, Doctor.  Violence.\\n\\ntext: COLE\\'S eyes return to the walls.\\nHeadlines:  \"CLOCK TICKING!  NO CURE YET!\"\\n\\nspeaker_heading: SCARFACE\\n\\ndialog: Anti-social six -- doing 25 to life.\\n\\nspeaker_heading: ENGINEER\\n\\ndialog: I don\\'t think he\\'s going to hurt us.  You\\'re\\n\\t\\tnot going to hurt us, are you Mr. Cole?\\n\\ntext: COLE\\'S head turns quickly to the ENGINEER.\\n\\nspeaker_heading: COLE\\n\\ndialog: No, sir.\\n\\ntext: The GUARDS exchange a look, shrug, exit, closing the door.\\n\\nspeaker_heading: MICROBIOLOGIST\\n\\ndialog: Why don\\'t you sit down, Mr. Cole.\\n\\ntext: COLE goes to the empty chair at the conference table, sits down.\\n\\nspeaker_heading: speaker_heading: \\n\\ndialog: We want you to tell us about last\\n\\t\\tnight.\\n\\nspeaker_heading: BOTANIST\\n\\ndialog: It\\'s important to observe everything.\\n\\nspeaker_heading: COLE\\n\\ndialog: I think it was...I\\'m sure it was 2nd Street.\\n\\ntext: As the SCIENTISTS start to whisper animatedly among themselves,\\n\\tCOLE\\'S eyes drift across the newspaper clippings taped to the\\n\\twall.  One headline screams, \"VIRUS MUTATING!\"  Another features\\n\\ta photo of an OLD MAN (DR. MASON, who we\\'ll see again later on)\\n\\tand the words, SCIENTIST SAYS, \"IT\\'S TOO LATE FOR CURE\".\\n\\nspeaker_heading: ASTROPHYSICIST\\'S VOICE (o.s.)\\n\\ndialog: Close your eyes, Cole.\\n\\ntext: Startled, COLE closes his eyes obediently.\\nBLACKNESS.  Like COLE, WE SEE NOTHING.  But we HEAR their VOICES.\\n\\nspeaker_heading: ENGINEER\\'S VOICE (o.s.)\\n\\ndialog: Tell us in detail what you\\'ve seen in\\n\\t\\tthis room.\\n\\nspeaker_heading: COLE\\'S VOICE (o.s.)\\n\\ndialog: Uh, in this room?  Uh...\\n\\nspeaker_heading: MICROBIOLOGIST\\'S VOICE (o.s.)\\n\\ndialog: How many of us are there?\\n\\nspeaker_heading: COLE\\'S VOICE (o.s.)\\n\\ndialog: Six...seven, if you count me.\\n\\nspeaker_heading: ASTROPHYSICIST\\'S VOICE (o.s.)\\n\\ndialog: Tell us about the pictures on the wall...\\n\\nspeaker_heading: COLE\\'S VOICE (o.s.)\\n\\ndialog: Uh, you mean the newspapers?\\n\\nspeaker_heading: A MONTAGE OF OVERLAPPING VOICES (o.s.)\\n\\ndialog: Tell us about the newspapers.  Can you\\n\\t\\thear my voice?  What do I look like?\\n\\t\\tWhat does he look like, the man who\\n\\t\\tjust spoke?  How old were you when you\\n\\t\\tleft the surface?\\n\\ntext: The VOICES blur into a cacophony and FADE INTO the droning P.A.\\n\\tSYSTEM at the airport.'}\n",
      "{'role': 'assistant', 'content': \"**Feedback on Scene:**\\n\\n**Clarity:**\\nThe scene effectively establishes a vivid contrast between two distinct settings: the airport terminal and the dystopian future world. The transition from Young Cole's perspective to the adult Cole's environment is clear, and the use of visual and auditory cues helps the audience follow the narrative shift. However, the scene could benefit from more explicit connections between the two timelines to enhance the audience's understanding of their relationship. The introduction of the scientists and their roles could be clarified further to avoid confusion about their significance and purpose.\\n\\n**Pacing:**\\nThe pacing of the scene is generally well-managed, with a gradual build-up from the initial airport incident to the more intense and mysterious future setting. The transition from the dream-like memory to the harsh reality of the future is smooth, maintaining the audience's engagement. However, the sequence in the future world, particularly the decontamination and specimen collection scenes, could be tightened to maintain momentum. Some of the detailed descriptions, while atmospheric, may slow down the narrative progression.\\n\\n**Dialogue:**\\nThe dialogue is functional and serves to reveal character and plot, but it could be more dynamic to enhance character development and engagement. The exchanges between Cole and the guards, as well as the scientists, are straightforward but lack emotional depth or tension that could add layers to Cole's character. The scientists' dialogue, while informative, could be more varied to distinguish their individual personalities and expertise. Additionally, the use of overlapping voices at the end is an interesting technique but might be overwhelming if not balanced with clearer individual lines to maintain coherence.\\n\\nOverall, the scene effectively sets up a mysterious and intriguing narrative, but could benefit from tighter pacing and more nuanced dialogue to fully engage the audience and clarify the connections between the past and future timelines.\"}\n",
      "--------------------------------------------------\n",
      "Processing file: fine_tuning_data.jsonl\n",
      "No formatting errors found in file fine_tuning_data.jsonl\n",
      "\n",
      "#### Distribution of total tokens:\n",
      "min / max: 991, 5673\n",
      "mean / median: 2487.0, 1936.0\n",
      "p5 / p95: 1253.4, 4403.0\n",
      "\n",
      "#### Distribution of assistant tokens:\n",
      "min / max: 308, 430\n",
      "mean / median: 375.969696969697, 375.0\n",
      "p5 / p95: 335.8, 423.0\n",
      "Dataset has ~12407 tokens that will be charged for during training\n",
      "You will train for 3 epochs on this dataset\n",
      "You will be charged for ~37221 tokens\n",
      "WARNING: Your dataset contains examples longer than 4K tokens by {n_billing_tokens_in_dataset - MAX_TOKENS} tokens.\n",
      "You will be charged for the full length of these examples during training, but only the first 4K tokens will be used for training.\n",
      "Processing file completed: fine_tuning_data.jsonl\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Listing the training and validation files\n",
    "    files = [\"fine_tuning_data.jsonl\"]\n",
    "\n",
    "    for file in files:\n",
    "        # Running basic checks on the files\n",
    "        if not basic_checks(file):\n",
    "            print(\"Exiting...\")\n",
    "            exit()\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Running additional checks to validate token counts and number of examples per label\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    files = [\n",
    "        \"fine_tuning_data.jsonl\",\n",
    "    ]\n",
    "    \n",
    "    # Performing token checks and formatting checks\n",
    "    for file in files:\n",
    "        print(f\"Processing file: {file}\")\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            dataset = [json.loads(line) for line in f]\n",
    "\n",
    "        total_tokens = []\n",
    "        assistant_tokens = []\n",
    "\n",
    "        if not format_checks(dataset, file):\n",
    "            print(\"Exiting...\")\n",
    "            exit()\n",
    "\n",
    "        for ex in dataset:\n",
    "            messages = ex.get(\"messages\", {})\n",
    "            total_tokens.append(num_tokens_from_messages(messages))\n",
    "            assistant_tokens.append(num_assistant_tokens_from_messages(messages))\n",
    "        \n",
    "        # Printing results of the checks\n",
    "        print_distribution(total_tokens, \"total tokens\")\n",
    "        print_distribution(assistant_tokens, \"assistant tokens\")\n",
    "        estimate_tokens(dataset, assistant_tokens)\n",
    "        print(f\"Processing file completed: {file}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a0ab4b",
   "metadata": {},
   "source": [
    "The checks for the data set returned nothing to be concerned with, save for maybe the token costs. Because the inputs and outputs are pretty hefty token-wise, I'll just need to watch my credits during and after the fine-tuning to make sure I am not running up too high of a bill. Otherwise, looks like the data preparation was fairly successful, and the data is ready to be used for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42801029",
   "metadata": {},
   "source": [
    "### Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1547c4",
   "metadata": {},
   "source": [
    "Now comes the time for fine-tuning! The first step of this process is to load in the training file and retrieve the training file ID. Once I have that, I can then construct the fine-tuning job. For this assignment, I will be using gpt-3.5-turbo as it is a little chepaer than gpt-4 and gpt-4o-mini might not be the best choice for this task. I will then construct a series of checks that will provide updates on the status of the fine-tuning job, both statically and dynamically. Once the fine-tuning job is successfully completed, we can then move on to testing the fine-tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a340f830",
   "metadata": {},
   "source": [
    "#### Loading the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f61d8d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FILENAME = 'fine_tuning_data.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d8509d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file ID: file-6wY9QydCcKddKKkv1qaosN\n",
      "Training file name: fine_tuning_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Loading in the training dataset\n",
    "file = client.files.create(\n",
    "  file=open(TRAINING_FILENAME, \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "print(\"Training file ID:\", file.id)\n",
    "print(\"Training file name:\", file.filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf3145f",
   "metadata": {},
   "source": [
    "#### Starting the Fine-Tuning Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9afe214",
   "metadata": {},
   "source": [
    "Now that we have the training file loaded and have an ID assigned to it, we can construct the fine-tuning job. I'll be using 3 epochs for this training because the training set isn't huge and I don't want to risk the change of overfitting the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e6299f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning job ID: ftjob-tYxjZTx3VCVc8GLr2KMx3WYT\n"
     ]
    }
   ],
   "source": [
    "# Starting the fine-tuning job\n",
    "ft = client.fine_tuning.jobs.create(\n",
    "    training_file=\"file-6wY9QydCcKddKKkv1qaosN\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    method={\n",
    "        \"type\": \"supervised\",\n",
    "        \"supervised\": {\n",
    "            \"hyperparameters\": {\n",
    "                \"n_epochs\": 3\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Finetuning job ID:\", ft.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4075ed2",
   "metadata": {},
   "source": [
    "#### Checking the Status of the Fine-Tuning Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b28e66",
   "metadata": {},
   "source": [
    "Now, we will perform a number of checks to keep tabs on the training process. First, I will grab a list of all of my queued up fine-tuning jobs and the status of each one to see where my current job falls in the line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcb81993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftjob-tYxjZTx3VCVc8GLr2KMx3WYT running\n",
      "ftjob-gWHovX1gmqLgXZapbrOZYzOu queued\n",
      "ftjob-DHf5S7JzXN0S2dVYn6Ge2nji succeeded\n"
     ]
    }
   ],
   "source": [
    "# Retrieving all queued fine-tuning jobs\n",
    "ft_jobs = client.fine_tuning.jobs.list()\n",
    "for ft_job in ft_jobs:\n",
    "    print(ft_job.id, ft_job.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dee5962",
   "metadata": {},
   "source": [
    "The next check will list out the the events that have been completed in the job so far. From this list, we can see that the job was started, the files validated, and the fine-tuning job was created and is currently in progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d65986f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftevent-pAvHAn1tm9zvL0iqR8NKH0dJ Fine-tuning job started\n",
      "ftevent-KANGvfJCoSWgeQpg3JiOSKji Files validated, moving job to queued state\n",
      "ftevent-5oBRI2I7yMmR4oqRjs0tMET4 Validating training file: file-6wY9QydCcKddKKkv1qaosN\n",
      "ftevent-R8TKRvUAZeD91iuXJd7969LA Created fine-tuning job: ftjob-tYxjZTx3VCVc8GLr2KMx3WYT\n"
     ]
    }
   ],
   "source": [
    "ft_job_events = client.fine_tuning.jobs.list_events(\n",
    "    fine_tuning_job_id='ftjob-tYxjZTx3VCVc8GLr2KMx3WYT',\n",
    "    limit=2)\n",
    "\n",
    "for ft_job_event in ft_job_events:\n",
    "    print(ft_job_event.id, ft_job_event.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c8ff98",
   "metadata": {},
   "source": [
    "The last checks will be a dynamic function that checks the status of the job every 10 seconds until the job either fails or is completed successfully. The status is updated after every check, and we can see with the final output that the job was carried out successfully. This is an extremely useful way to keep tabs on the fine-tuning job and get real-time updates as to where you are at in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56fbabd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning job ftjob-tYxjZTx3VCVc8GLr2KMx3WYT finished with status: succeeded\n"
     ]
    }
   ],
   "source": [
    "JOB_ID = \"ftjob-tYxjZTx3VCVc8GLr2KMx3WYT\"\n",
    "\n",
    "url = f\"https://api.openai.com/v1/fine_tuning/jobs/{JOB_ID}\"\n",
    "\n",
    "# Defining headers for authentication\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Tracking the progress of the fine-tuning job\n",
    "start_time = time.time()\n",
    "status = None\n",
    "\n",
    "while status not in [\"succeeded\", \"failed\"]:\n",
    "    # Making a GET request to retrieve the job details\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        job_details = response.json()\n",
    "        status = job_details.get(\"status\")\n",
    "        \n",
    "        # Printing job details\n",
    "        print(job_details)\n",
    "        \n",
    "        print(\"Elapsed Time: {} minutes {} seconds\".format(\n",
    "            int((time.time() - start_time) // 60),\n",
    "            int((time.time() - start_time) % 60)))\n",
    "        print(f'Status: {status}')\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        break \n",
    "    \n",
    "    # Providing a check every 10 seconds and clearing the previous output\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(10)  \n",
    "\n",
    "# Printing final status\n",
    "if status in [\"succeeded\", \"failed\"]:\n",
    "    print(f'Fine-Tuning job {JOB_ID} finished with status: {status}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05a014b",
   "metadata": {},
   "source": [
    "Now that the the fine-tuning job is complete, it is time to test out the model and see how well it does!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c3e1c",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3778eecb",
   "metadata": {},
   "source": [
    "Now comes the really fun part - testing the fine-tuned model to see how the fine-tuning has improved the performance of the model with providing feedback for screenwriters. I am going to use some of the prompts that I used in my initial testing in the last milestone to see how the responses have improved from the base gpt-3.5-turbo model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3dd3987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-tYxjZTx3VCVc8GLr2KMx3WYT', created_at=1739040309, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-3.5-turbo-0125:personal::AykBr6aP', finished_at=1739040597, hyperparameters=Hyperparameters(batch_size=1, learning_rate_multiplier=2.0, n_epochs=3), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-MktBuv68z9JVLPqQFWPQVoIe', result_files=['file-Wm9PEdU9qf7oE6tpg9gud5'], seed=2039927102, status='succeeded', trained_tokens=246015, training_file='file-6wY9QydCcKddKKkv1qaosN', validation_file=None, estimated_finish=None, integrations=[], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=1, learning_rate_multiplier=2.0, n_epochs=3)), type='supervised'), user_provided_suffix=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieving details of the fine-tuning job\n",
    "client.fine_tuning.jobs.retrieve(\"ftjob-tYxjZTx3VCVc8GLr2KMx3WYT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f0455f",
   "metadata": {},
   "source": [
    "#### Test 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffa7663",
   "metadata": {},
   "source": [
    "This first prompt is rather basic to see how the model performs with the base, expected instructions, but it does one of the non copyrighted scripts from the initial testing so we can compare some of the feedback that it provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bfd88858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Feedback on Scene:**\n",
      "\n",
      "**Clarity:**\n",
      "Overall, the scene is clear and easy to follow. The setting and characters are well-established, and the dialogue effectively conveys the central conflict. The use of the dog park as a backdrop adds a nice visual element and helps to ground the conversation. The transition from the anecdote about the date to the revelation about Ted's profile is smooth and natural. The interaction between Ben and Vera is clear, and their friendship is evident through their comfortable banter.\n",
      "\n",
      "**Pacing:**\n",
      "The pacing of the scene is well-managed. The initial setup, with Ben recounting his date, is engaging and sets a light-hearted tone. The tension builds gradually as Ben describes Ted's behavior, leading to a humorous and satisfying reveal with Vera's discovery of Ted's profile. The scene's conclusion, with Ben's realization and joking response, provides a nice resolution. The dialogue is snappy, which keeps the scene moving without feeling rushed.\n",
      "\n",
      "**Dialogue:**\n",
      "The dialogue is one of the scene's strengths. It is witty and natural, capturing the dynamic between Ben and Vera effectively. Ben's recounting of the date is humorous, and his reactions to Ted's behavior are relatable. Vera's responses are supportive and playful, adding a warm touch to the conversation. The dialogue also serves to reveal character traits, such as Ben's attachment to Lupin and Vera's quick thinking. The humor is well-balanced, with a good mix of situational comedy and character-driven jokes.\n",
      "\n",
      "**Additional Thoughts:**\n",
      "The scene successfully blends humor with a touch of tension, making it engaging and entertaining. The use of the dog as a catalyst for the story is clever and adds a unique element. The scene could benefit from a bit more physical description to enhance the setting and character actions, though the dialogue carries the scene well on its own. Consider using stage directions to highlight Ben's reactions to Ted's behavior or Vera's playful gestures, which could further enhance the visual and emotional impact of the scene.\n",
      "\n",
      "Overall, this scene is well-crafted, with strong dialogue and a fun premise. With some additional visual and character detailing, it has the potential to be a memorable and enjoyable moment in a larger narrative.\n"
     ]
    }
   ],
   "source": [
    "prompt_one = \"\"\"\n",
    "EXT. DOG PARK - DAY\n",
    "BEN and VERA sit on a park bench in a trendy dog park\n",
    "watching BEN’s dog Lupin run around in circles.\n",
    "VERA\n",
    "You never told me how your date\n",
    "went the other night.\n",
    "BEN\n",
    "It was … actually a little strange.\n",
    "You know how Lupin is in a few of\n",
    "my profile pictures, right? On the\n",
    "app?\n",
    "VERA\n",
    "From what you’ve told me, Lupin is\n",
    "an excellent wing-dog.\n",
    "BEN\n",
    "He’s the goodest boy there is.\n",
    "VERA\n",
    "So what happened?\n",
    "BEN\n",
    "So the other night, I’m out with\n",
    "this guy … “Ted”. We had a great\n",
    "time: drinks at that new place with\n",
    "the secret door, dinner at La Mer\n",
    "and then I suggested a night cap\n",
    "back at mine. We get back to mine\n",
    "and Ted meets Lupin and is all over\n",
    "him--big cuddles, throwing toys\n",
    "down the hall. Like I suddenly\n",
    "didn’t exist.\n",
    "VERA\n",
    "Maybe you’re Lupin’s wingman, did\n",
    "you ever think about that?\n",
    "BEN\n",
    "But Ted hadn’t mentioned him all\n",
    "night! Most guys I’m out with,\n",
    "that’s like the first thing they\n",
    "bring up. Lupin’s a great\n",
    "icebreaker, that’s why I’ve got him\n",
    "in so many of my pictures.\n",
    "(MORE)\n",
    "BEN (CONT'D)\n",
    "2.\n",
    "This guy shows up at my place and\n",
    "it’s like he just happens to\n",
    "discover that I’m a dog owner?\n",
    "VERA\n",
    "Doesn’t really sound like much of a\n",
    "crime, Ben.\n",
    "BEN\n",
    "Then: I go to the kitchen to get a\n",
    "bottle of wine, and when I come\n",
    "back Ted’s taking all these photos\n",
    "with Lupin. Hugging him, patting\n",
    "him, staring in his eyes. I swear I\n",
    "caught him draping his scarf around\n",
    "Lupin’s neck. I had to fake a\n",
    "headache and get him out of there.\n",
    "Like, that is weird, right?\n",
    "Pause. VERA shifts her weight on the park bench.\n",
    "VERA\n",
    "Do you still have him on the app?\n",
    "BEN forks over his phone. VERA scrolls for a moment and then\n",
    "gasps.\n",
    "VERA (CONT'D)\n",
    "Oh. My. God. Look!\n",
    "“Ted” has a different dog in every picture on his profile.\n",
    "BEN swipes through them in horror, settling on one in which\n",
    "Lupin wears a scarf. He looks over to the dog, oblivious,\n",
    "running around the park with a stick in his mouth.\n",
    "BEN\n",
    "He’s a monster. Some kind of serialkiller-dog-stalker.\n",
    "VERA\n",
    "You’re so lucky. Both of you!\n",
    "BEN\n",
    "I need to hug Lupin. (Calling out.)\n",
    "Lupin! LUPIN!\n",
    "He runs to his dog across the grass.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Testing the fine-tuned model\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"ft:gpt-3.5-turbo-0125:personal::AykBr6aP\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a focus group reviewing a scene script for an unproduced film or television show. Provide constructive feedback on clarity, pacing, and dialogue. Consider engagement, emotional impact, and storytelling effectiveness in your analysis.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Here is a script scene:\\n\\n{prompt_one}\\n\\nPlease provide your feedback.\"}\n",
    "  ],\n",
    "  max_tokens=500\n",
    ")\n",
    "     \n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3af99d6",
   "metadata": {},
   "source": [
    "Immediately I can see an improvement from the initial outputs from the last milestone. Whreas the base model provided rather surface-level feedback, the fine-tuned model gives specific feedback on whaht parts of the script are strong and where some improvements can be made. The output follows the structure that the fine-tuning data outlined, giving feedback on pacing, clarity, and dialogue. It was also interesting to see that the model also provided some additional thoughts as well with some expanded feedback but aso added some great feedback on ways to make the scene stronger. Overall, I'd say so far that this model is a significant improvement over the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de040ef",
   "metadata": {},
   "source": [
    "#### Test 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e955ad",
   "metadata": {},
   "source": [
    "This next prompt iis word for word one of the test prompts I tried in Milestone 2, providing another script snippet to the model and asking specifically for feedback regarding how the scene's dialogue would hit with a Gen Z audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1738e4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Feedback on Scene:**\n",
      "\n",
      "**Clarity:**\n",
      "The scene is clear and effectively establishes the setting, the characters' relationship, and the tension leading up to the midnight kiss. The use of the balcony at a New Year's Eve party as the backdrop is a familiar and relatable setting, and it provides a nice contrast between the intimate moment between Ash and Charlie and the bustling party inside.\n",
      "\n",
      "**Pacing:**\n",
      "The pacing is well-executed, with a gradual build-up as Charlie introduces the idea of the kiss at midnight. The dialogue exchange is lively and keeps the audience engaged, with a nice balance of tension and humor. The countdown at the end adds a sense of urgency and anticipation, leading to a satisfying conclusion.\n",
      "\n",
      "**Dialogue:**\n",
      "The dialogue is natural and character-driven, effectively conveying the dynamic between Ash and Charlie. Charlie's playful and slightly flirtatious tone contrasts nicely with Ash's more reserved and skeptical demeanor. The back-and-forth about the kiss is humorous and relatable, capturing the awkwardness and excitement of the moment.\n",
      "\n",
      "**Suggestions for Tailoring to Gen Z Audience:**\n",
      "\n",
      "**1. Incorporate more slang or colloquial language:**\n",
      "- You could have Charlie use more casual language, like \"Hey, so, like, check it out,\" instead of \"I’ve been thinking.\"\n",
      "- You might add a moment where Ash says something like, \"Wait, are you for real?\" to enhance the relatability for a younger audience.\n",
      "\n",
      "**2. Highlight technology:**\n",
      "- Consider having one of the characters check their phone for the time or mention a social media post related to New Year's Eve plans.\n",
      "- Integrate a joke about a popular dating app or social media trend to make the conversation more current.\n",
      "\n",
      "**3. Play with pop culture references:**\n",
      "- Charlie could make a joke about kissing being like a scene from a romantic comedy, using a reference from a recent popular film or show.\n",
      "- Integrate a brief exchange about a trending meme or viral video to add a contemporary touch to the dialogue.\n",
      "\n",
      "**Specific Example:**\n",
      "CHARLIE: \"Yo, so like, picture this. Midnight hits, we lock lips... no drama, just vibes.\"\n",
      "ASH: \"Wait, seriously? You want us to do the whole countdown kiss thing?\"\n",
      "CHARLIE: \"Yeah, it's like that classic rom-com move, but with us. Minus the cheesy soundtrack, though.\"\n",
      "\n",
      "**Incorporated Gen Z Elements:**\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "prompt_two = \"\"\"\n",
    "EXT. APARTMENT BALCONY - NIGHT\n",
    "ASH checks their watch: just minutes left until midnight.\n",
    "They look up and out at the city before them and sigh through\n",
    "their nose.\n",
    "They’re standing on the large, wraparound balcony of a nice\n",
    "apartment. Inside, a New Year’s Eve party is in full swing.\n",
    "Out here, in the cold, various guests have stepped out for\n",
    "various reasons: for fresh air, for a smoke, for a break from\n",
    "the crowd, for privacy with one another. ASH does their best\n",
    "to ignore the lot of them--to keep their distance. They check\n",
    "their watch again.\n",
    "The sounds of the party crescendo for a moment as the sliding\n",
    "balcony door is quickly opened and closed. Out of the party\n",
    "steps ASH’s friend CHARLIE, carrying a bottle of champagne\n",
    "and two glasses. CHARLIE joins ASH by the railing.\n",
    "CHARLIE\n",
    "(pouring drinks)\n",
    "I’ve got it!\n",
    "ASH\n",
    "(taking a glass)\n",
    "Nice! What have you got?\n",
    "CHARLIE\n",
    "I’ve been thinking.\n",
    "ASH\n",
    "You’ve been thinking...\n",
    "CHARLIE\n",
    "It’s a thought. A bold thought.\n",
    "ASH\n",
    "A New Year’s resolution?\n",
    "CHARLIE\n",
    "God, no! It’ll be far too late by\n",
    "then.\n",
    "ASH\n",
    "So let’s hear it.\n",
    "CHARLIE\n",
    "Not yet: first, we drink.\n",
    "They ‘cheers!’ And drink. CHARLIE pours another round.\n",
    "2.\n",
    "ASH\n",
    "So far, so good.\n",
    "CHARLIE\n",
    "This isn’t even “the thing”.\n",
    "ASH\n",
    "So what is “the thing”?\n",
    "CHARLIE\n",
    "Drink again and I’ll tell you.\n",
    "ASH drinks. CHARLIE doesn’t.\n",
    "CHARLIE (CONT'D)\n",
    "I think we should kiss at midnight.\n",
    "Pause. ASH performs the longest drink swallow of their life.\n",
    "CHARLIE fills ASH’s glass again.\n",
    "ASH\n",
    "You and me?\n",
    "CHARLIE\n",
    "Yep.\n",
    "ASH\n",
    "You want to kiss?\n",
    "CHARLIE\n",
    "At midnight. For New Year’s.\n",
    "ASH\n",
    "I’m confused.\n",
    "CHARLIE\n",
    "Don’t be. It’s a tradition, whereASH\n",
    "No, I get that bit. You want us to\n",
    "kiss at midnight?\n",
    "CHARLIE\n",
    "I figure with you, I’m guaranteed a\n",
    "kiss. If I look elsewhere, I might\n",
    "not be so lucky.\n",
    "ASH\n",
    "So it’s insurance? Isn’t that guy\n",
    "here? That one you want to...\n",
    "CHARLIE\n",
    "Nathan. Yes, Nathan is here. And I\n",
    "thought about shooting that shotASH\n",
    "But instead2.\n",
    "3.\n",
    "CHARLIE\n",
    "...I came out here.\n",
    "ASH\n",
    "For me?\n",
    "CHARLIE\n",
    "Little bit wishing I hadn’t, now...\n",
    "An OBNOXIOUS GUEST bangs on the glass door and mouths “GET\n",
    "READY!” Charlie downs their glass and pours another.\n",
    "ASH\n",
    "What are you doing?\n",
    "CHARLIE\n",
    "Catching up with you.\n",
    "ASH\n",
    "You know that’s not what I meant.\n",
    "A “WHOOP!” from the crowd inside. CHARLIE smiles at ASH.\n",
    "CHARLIE\n",
    "It might be nice?\n",
    "ASH\n",
    "I think it could be.\n",
    "CHARLIE\n",
    "Not where I’d planned the evening\n",
    "was gonna go...\n",
    "ASH\n",
    "You can still go and find Nathan.\n",
    "CHARLIE shakes their head. Inside the apartment, a muffled\n",
    "countdown begins: *TEN*, *NINE*, *EIGHT*, *SEVEN*...\n",
    "CHARLIE\n",
    "So are we doing it?\n",
    "ASH\n",
    "Is it a good idea?\n",
    "CHARLIE\n",
    "That sounds like next year’s\n",
    "problem.\n",
    "ASH\n",
    "Just a kiss?\n",
    "CHARLIE\n",
    "Maybe?\n",
    "... *THREE*, *TWO*, *ONE*! The party erupts. Champagne,\n",
    "cheering, laughter, hugs, kisses. And ASH and CHARLIE\n",
    "together in the middle of it all.\n",
    "\"\"\"\n",
    "\n",
    "# Testing the fine-tuned model\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"ft:gpt-3.5-turbo-0125:personal::AykBr6aP\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a focus group reviewing a scene script for an unproduced film or television show. Provide constructive feedback on clarity, pacing, and dialogue. Consider engagement, emotional impact, and storytelling effectiveness in your analysis.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Here is a script scene:\\n\\n{prompt_two}\\n\\nPlease provide your feedback. Give suggestions on how to tailor the dialogue between the characters more to a Gen Z audience. Give specific examples of where you would incorporate some of these suggestions in the script.\"}\n",
    "  ],\n",
    "  max_tokens=500\n",
    ")\n",
    "     \n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b7cb5a",
   "metadata": {},
   "source": [
    " If you recall, the feedback from the base model was cringeworthy, leaning heavily into Gen Z steeotypes to the point where the suggestions were comical. The fine-tuned model shows a significant improvement. While it still provides the feedback on clarity, pace, and dialogue, it both tailors that feedback and provides additional feedback on how to incorporate more elements of Gen Z culture into the scene. It even provided some specific dialogue suggestions whcih, while a little rough around the edges, are much better than the base model suggestions. The fine-tuned model also was much better at correctly identifying the different parts of the script, most likely from the labeled scripts that was provided to it during fine-tuning, and was able to differentiate dialogue from stage direction and so on. This prompt showed heavy improvements from the output of the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336b5803",
   "metadata": {},
   "source": [
    "#### Test 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced2bb48",
   "metadata": {},
   "source": [
    "Last, I am going to try giving the model a concept of a short film as opposed to a whole script and ask for some feedback on the story idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4638736b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Character Development:**\n",
      "The character development in this short film concept appears to revolve around the growth and transformation of the artist protagonist. This is a compelling arc, as many audience members can likely relate to the struggles of creativity and inspiration. To enhance the character development, consider delving deeper into the protagonist's internal world. What specific obstacles or fears are they facing in their artistic process? What is at stake for them personally and professionally? By fleshing out these internal conflicts, the audience can better understand and empathize with the protagonist's journey.\n",
      "\n",
      "Additionally, you might consider incorporating a backstory or a glimpse into the protagonist's past to provide context for their current struggles. This can add layers to the character and make their eventual transformation even more resonant. Whether through a brief flashback or a revealing conversation, a well-crafted backstory can enrich the audience's connection to the protagonist.\n",
      "\n",
      "**Story Pacing:**\n",
      "The pacing of the story is vital in a short film, where every moment counts. In this concept, the pacing should be carefully calibrated to reflect the protagonist's emotional journey. At the beginning, the pacing can be slow and contemplative, mirroring the artist's creative block and sense of stagnation. This deliberate pacing allows the audience to feel the weight of the protagonist's struggle and sets the stage for their eventual breakthrough.\n",
      "\n",
      "As the story progresses and the artist's perspective shifts, the pacing should gradually quicken to convey their growing excitement and inspiration. The film can use editing techniques, such as montage sequences or dynamic camera movements, to visually represent this change in tempo. The climax, where the artist translates their newfound inspiration into art, should be the most fast-paced and visually striking moment, capturing the exhilaration of creative discovery.\n",
      "\n",
      "Overall, ensuring a balance between the reflective, slower moments and the more energetic, transformative scenes will help maintain the audience's engagement and emotional investment in the protagonist's journey.\n"
     ]
    }
   ],
   "source": [
    "prompt_three = '''\n",
    "Imagine a short film where the protagonist is a young artist struggling to find inspiration. The film follows the protagonist \n",
    "as they go about their day to day life seemingly in a creative slump. However, something happens that opens the artist's eyes\n",
    "up to the wonder in the ordinary, and we watch as the artist turns their ordinary experiences into extraordinary art.\n",
    "'''\n",
    "\n",
    "# Testing the fine-tuned model\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"ft:gpt-3.5-turbo-0125:personal::AykBr6aP\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a focus group reviewing a scene script for an unproduced film or television show. Provide constructive feedback on clarity, pacing, and dialogue. Consider engagement, emotional impact, and storytelling effectiveness in your analysis.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Here is a synopsis for a short film:\\n\\n{prompt_three}\\n\\nDescribe some feedback you might give on the character development and story pacing.\"}\n",
    "  ],\n",
    "  max_tokens=500\n",
    ")\n",
    "     \n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a22939b",
   "metadata": {},
   "source": [
    "This prompt was interesting because I only provided scripts in the fine-tuning examples, not synopses, but I wanted to see if the generalization abilities of the base model were still present in this fine-tuned model. Sure enough, the fine-tuned model performed relatively well with working with what I gave it. It also strayed a bit from the clarity, dialogue, pacing formula and provide feedback relevant to what I was looking for. This skill is critical in creative tasks like this as it allows the model to adapt to different aspects of storytelling without being overly rigid in its responses. Rather than simply mimicking the structure of the fine-tuning examples, the model demonstrated an ability to extrapolate and provide insightful, context-aware feedback. This suggests that fine-tuning can enhance the model’s specificity while still preserving the flexibility and generalization capabilities of the base model - an important balance for applications that require both structure and creative interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a76fcf",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45976d8f",
   "metadata": {},
   "source": [
    "Overall, fine-tuning significantly improved the quality of feedback, enhancing the model’s ability to understand the nuances of script structure and provide more insightful, context-aware responses. It was also seen that the model retained a good amount of adaptability despite the structured examples provided in the training set, allowing it to go beyond rigid criteria and offer relevant feedback tailored to each scene—a crucial skill for creative applications. Moving forward, I would love to further refine and expand on the training data to add some more examples with human-generated feedbcak to further inprove the quality of the feedback. Adding in some data on how real films and television shows end up performing (or real responses from focus groups) could also be useful in identifying audience trends, refining the model’s feedback to align more closely with what resonates with viewers. If the model could recognize patterns in successful scripts, like pacing choices, character dynamics, or dialogue styles that tend to engage audiences, it could be incredibly useful to elevate the feedback to another level. Groeing the training data would also allow for the creation of a test set to be able to quantify improvements more systematically. Right now, the evaluation is mostly qualitative—observing how the feedback feels rather than measuring specific performance gains. With a proper test set, I could start tracking metrics like consistency, relevance, and even sentiment alignment with professional script coverage. . Ultimately, this project has shown that with the right data and approach, language models can become valuable creative tools not just for providing feedback, but in shaping stronger, more engaging stories."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
